{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "embedding_size = 100 # Dimension of the embedding vector.\n",
    "\n",
    "num_sampled = 5 # Number of negative examples to sample.\n",
    "sentence_len = 5\n",
    "vocabulary_size = 10000\n",
    "label_size = 19\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, sentence_len])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([label_size, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([label_size]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  # embedding_lookup()用法: http://blog.csdn.net/u013041398/article/details/60955847\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  sentence_embed = tf.reduce_mean(embed, axis=1)\n",
    "  print(sentence_embed.shape)\n",
    "  print(softmax_weights.shape)\n",
    "  logits = tf.matmul(sentence_embed, tf.transpose(softmax_weights)) + softmax_biases\n",
    "\n",
    "  loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                tf.nn.nce_loss(weights=softmax_weights,  #[embed_size, label_size]--->[label_size,embed_size]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n",
    "                               biases=softmax_biases,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n",
    "                               labels=train_labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n",
    "                               inputs=sentence_embed,# [None,self.embed_size] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                               num_sampled=num_sampled,  #scalar. 100\n",
    "                               num_classes=label_size,partition_strategy=\"div\"))\n",
    "        \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0, trainable=False)\n",
    "  learning_rate = tf.train.exponential_decay(0.01, global_step, 1000, 0.9, staircase=True)\n",
    "  optimizer = tf.contrib.layers.optimize_loss(loss, global_step=global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "\n",
    "  train_predictions = tf.argmax(logits, axis=1)  # shape:[None,]\n",
    "  correct_prediction = tf.equal(tf.cast(train_predictions, tf.int32), tf.reshape(train_labels, [-1]))#tf.argmax(self.logits, 1)-->[batch_size]\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # shape=()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "\n",
    "  for i in range(num_steps):\n",
    "    d = np.zeros((batch_size,sentence_len),dtype=np.int32) #[None, self.sequence_length]\n",
    "    bl = bl = np.array([1,0,1,1,1,2,1,1],dtype=np.int32).reshape([batch_size,1]) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n",
    "    feed_dict = {train_dataset: d, train_labels: bl}\n",
    "    _, l,tp, acc = session.run([optimizer, loss, train_predictions, accuracy], feed_dict=feed_dict)\n",
    "    #print(\"loss:\",l,\"acc:\",a,\"label:\",batch_labels,\"prediction:\",p)\n",
    "    print(\"loss:\", l, \"prediction\", tp, \"accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started...\n",
      "loss: 7.38377 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 7.63799 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 6.40695 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 7.06332 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 7.95368 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 5.68974 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 6.41768 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 7.59212 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 6.18963 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.54854 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 7.25876 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 5.79528 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 5.19427 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.7056 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.76746 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.65689 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 5.09852 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 3.33876 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.44433 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 5.79343 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 2.99821 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 6.49452 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 5.50788 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.98451 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 3.1975 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 4.21356 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 2.93502 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 6.26054 acc: 0.0 label: [1 0 1 1 1 2 1 1] prediction: [11 11 11 11 11 11 11 11]\n",
      "loss: 3.55116 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.48693 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.46408 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.87946 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 4.03143 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.12275 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.92689 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.61604 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.19591 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.86099 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.4999 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.27555 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.39666 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 3.63865 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.9119 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.1291 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.81783 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.63798 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.21214 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.08732 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.11277 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.13264 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.58612 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.76392 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.93678 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.82908 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.89893 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.66348 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.03604 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.49108 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.81594 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.06438 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.25917 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.87811 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.27743 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.3262 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.28425 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.18467 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.00617 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.21245 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.97044 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.30224 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.92932 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.88447 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.15259 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.28686 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.33066 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.02488 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.988738 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.15609 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.21938 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.2461 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.14671 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.886457 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.01153 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 0.987899 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.95859 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.1866 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.12579 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.0078 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.01689 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.07358 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.24387 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.89341 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.9664 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.25557 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.75587 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.85582 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.88953 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 1.19146 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.03131 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "loss: 2.01842 acc: 0.75 label: [1 0 1 1 1 2 1 1] prediction: [1 1 1 1 1 1 1 1]\n",
      "ended...\n"
     ]
    }
   ],
   "source": [
    "# fast text. using: very simple model;n-gram to captrue location information;h-softmax to speed up training/inference\n",
    "# for the n-gram you can use data_util to generate. see method process_one_sentence_to_get_ui_bi_tri_gram under aa1_data_util/data_util_zhihu.py\n",
    "print(\"started...\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class fastTextB:\n",
    "    def __init__(self, label_size, learning_rate, batch_size, decay_steps, decay_rate,num_sampled,sentence_len,vocab_size,embed_size,is_training):\n",
    "        \"\"\"init all hyperparameter here\"\"\"\n",
    "        # set hyperparamter\n",
    "        self.label_size = label_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.sentence_len=sentence_len\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_size=embed_size\n",
    "        self.is_training=is_training\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        # add placeholder (X,label)\n",
    "        self.sentence = tf.placeholder(tf.int32, [None, self.sentence_len], name=\"sentence\")  # X\n",
    "        self.labels = tf.placeholder(tf.int32, [None], name=\"Labels\")  # y\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "        self.epoch_step=tf.Variable(0,trainable=False,name=\"Epoch_Step\")\n",
    "        self.epoch_increment=tf.assign(self.epoch_step,tf.add(self.epoch_step,tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name=\"Epoch_Step\")\n",
    "        self.instantiate_weights()\n",
    "        self.logits = self.inference() #[None, self.label_size]\n",
    "        if not is_training:\n",
    "            return\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "        self.predictions = tf.argmax(self.logits, axis=1, name=\"predictions\")  # shape:[None,]\n",
    "        correct_prediction = tf.equal(tf.cast(self.predictions,tf.int32), self.labels) #tf.argmax(self.logits, 1)-->[batch_size]\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\") # shape=()\n",
    "\n",
    "    def instantiate_weights(self):\n",
    "        \"\"\"define all weights here\"\"\"\n",
    "        # embedding matrix\n",
    "        self.Embedding = tf.get_variable(\"Embedding\", [self.vocab_size, self.embed_size])\n",
    "        self.W = tf.get_variable(\"W\", [self.embed_size, self.label_size])\n",
    "        self.b = tf.get_variable(\"b\", [self.label_size])\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"main computation graph here: 1.embedding-->2.average-->3.linear classifier\"\"\"\n",
    "        # 1.get emebedding of words in the sentence\n",
    "        sentence_embeddings = tf.nn.embedding_lookup(self.Embedding,self.sentence)  # [None,self.sentence_len,self.embed_size]\n",
    "\n",
    "        # 2.average vectors, to get representation of the sentence\n",
    "        self.sentence_embeddings = tf.reduce_mean(sentence_embeddings, axis=1)  # [None,self.embed_size]\n",
    "\n",
    "        # 3.linear classifier layer\n",
    "        logits = tf.matmul(self.sentence_embeddings, self.W) + self.b #[None, self.label_size]==tf.matmul([None,self.embed_size],[self.embed_size,self.label_size])\n",
    "        return logits\n",
    "\n",
    "    def loss(self,l2_lambda=0.01): #0.0001-->0.001\n",
    "        \"\"\"calculate loss using (NCE)cross entropy here\"\"\"\n",
    "        # Compute the average NCE loss for the batch.\n",
    "        # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "        # time we evaluate the loss.\n",
    "        if self.is_training: #training\n",
    "            labels=tf.reshape(self.labels,[-1])               #[batch_size,1]------>[batch_size,]\n",
    "            labels=tf.expand_dims(labels,1)                   #[batch_size,]----->[batch_size,1]\n",
    "            loss = tf.reduce_mean( #inputs: A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                tf.nn.nce_loss(weights=tf.transpose(self.W),  #[embed_size, label_size]--->[label_size,embed_size]. nce_weights:A `Tensor` of shape `[num_classes, dim].O.K.\n",
    "                               biases=self.b,                 #[label_size]. nce_biases:A `Tensor` of shape `[num_classes]`.\n",
    "                               labels=labels,                 #[batch_size,1]. train_labels, # A `Tensor` of type `int64` and shape `[batch_size,num_true]`. The target classes.\n",
    "                               inputs=self.sentence_embeddings,# [None,self.embed_size] #A `Tensor` of shape `[batch_size, dim]`.  The forward activations of the input network.\n",
    "                               num_sampled=self.num_sampled,  #scalar. 100\n",
    "                               num_classes=self.label_size,partition_strategy=\"div\"))  #scalar. 1999\n",
    "        else:#eval/inference\n",
    "            #logits = tf.matmul(self.sentence_embeddings, tf.transpose(self.W)) #matmul([None,self.embed_size])--->\n",
    "            #logits = tf.nn.bias_add(logits, self.b)\n",
    "            labels_one_hot = tf.one_hot(self.labels, self.label_size) #[batch_size]---->[batch_size,label_size]\n",
    "            #sigmoid_cross_entropy_with_logits:Computes sigmoid cross entropy given `logits`.Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive.  For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time.\n",
    "            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_one_hot,logits=self.logits) #labels:[batch_size,label_size];logits:[batch, label_size]\n",
    "            print(\"loss0:\", loss) #shape=(?, 1999)\n",
    "            loss = tf.reduce_sum(loss, axis=1)\n",
    "            print(\"loss1:\",loss)  #shape=(?,)\n",
    "        l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'bias' not in v.name]) * l2_lambda\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"based on the loss, use SGD to update parameter\"\"\"\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,learning_rate=learning_rate, optimizer=\"Adam\")\n",
    "        return train_op\n",
    "\n",
    "#test started\n",
    "def test():\n",
    "    #below is a function test; if you use this for text classifiction, you need to tranform sentence to indices of vocabulary first. then feed data to the graph.\n",
    "    num_classes=19\n",
    "    learning_rate=0.01\n",
    "    batch_size=8\n",
    "    decay_steps=1000\n",
    "    decay_rate=0.9\n",
    "    sequence_length=5\n",
    "    vocab_size=10000\n",
    "    embed_size=100\n",
    "    is_training=True\n",
    "    dropout_keep_prob=1\n",
    "    fastText=fastTextB(num_classes, learning_rate, batch_size, decay_steps, decay_rate,5,sequence_length,vocab_size,embed_size,is_training)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for i in range(100):\n",
    "            input_x=np.zeros((batch_size,sequence_length),dtype=np.int32) #[None, self.sequence_length]\n",
    "            input_y=input_y=np.array([1,0,1,1,1,2,1,1],dtype=np.int32) #np.zeros((batch_size),dtype=np.int32) #[None, self.sequence_length]\n",
    "            loss,acc,predict,_=sess.run([fastText.loss_val,fastText.accuracy,fastText.predictions,fastText.train_op],\n",
    "                                        feed_dict={fastText.sentence:input_x,fastText.labels:input_y})\n",
    "            print(\"loss:\",loss,\"acc:\",acc,\"label:\",input_y,\"prediction:\",predict)\n",
    "           \n",
    "test()\n",
    "print(\"ended...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
